# Logstash Configuration for RemoteHive
# Processes logs from various services and forwards to Elasticsearch

input {
  # Beats input for Filebeat
  beats {
    port => 5044
    type => "beats"
  }
  
  # Syslog input for system logs
  syslog {
    port => 5514
    type => "syslog"
  }
  
  # TCP input for application logs
  tcp {
    port => 5000
    type => "tcp"
    codec => json_lines
  }
  
  # UDP input for lightweight logs
  udp {
    port => 5001
    type => "udp"
    codec => json_lines
  }
  
  # HTTP input for webhook logs
  http {
    port => 8080
    type => "http"
  }
  
  # Redis input for queued logs
  redis {
    host => "redis"
    port => 6379
    key => "logstash"
    data_type => "list"
    type => "redis"
  }
  
  # File input for local log files
  file {
    path => "/var/log/remotehive/*.log"
    start_position => "beginning"
    type => "file"
    codec => multiline {
      pattern => "^%{TIMESTAMP_ISO8601}"
      negate => true
      what => "previous"
    }
  }
}

filter {
  # Add common fields
  mutate {
    add_field => {
      "[@metadata][pipeline]" => "remotehive"
      "environment" => "${ENVIRONMENT:development}"
      "cluster" => "${CLUSTER_NAME:remotehive-cluster}"
    }
  }
  
  # Parse timestamp
  if [timestamp] {
    date {
      match => [ "timestamp", "ISO8601" ]
      target => "@timestamp"
    }
  }
  
  # Process different log types
  if [type] == "beats" {
    # Filebeat logs processing
    if [fields][service] {
      mutate {
        add_field => { "service" => "%{[fields][service]}" }
      }
    }
    
    # Parse container logs
    if [container] {
      mutate {
        add_field => {
          "container_name" => "%{[container][name]}"
          "container_id" => "%{[container][id]}"
        }
      }
    }
  }
  
  # Backend API logs
  if [service] == "backend" or [container_name] =~ /backend/ {
    # Parse FastAPI logs
    grok {
      match => {
        "message" => "%{TIMESTAMP_ISO8601:timestamp} - %{WORD:log_level} - %{DATA:logger} - %{GREEDYDATA:log_message}"
      }
    }
    
    # Parse HTTP access logs
    if [log_message] =~ /^\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}/ {
      grok {
        match => {
          "log_message" => "%{IPORHOST:client_ip} - - \[%{HTTPDATE:access_time}\] \"%{WORD:http_method} %{URIPATH:uri_path}(?:%{URIPARAM:uri_params})? HTTP/%{NUMBER:http_version}\" %{INT:status_code} %{INT:response_size} \"%{DATA:referrer}\" \"%{DATA:user_agent}\" %{NUMBER:response_time}"
        }
      }
      
      mutate {
        convert => {
          "status_code" => "integer"
          "response_size" => "integer"
          "response_time" => "float"
        }
      }
    }
    
    # Parse JSON logs
    if [log_message] =~ /^\{/ {
      json {
        source => "log_message"
        target => "json_data"
      }
    }
    
    mutate {
      add_field => { "service_type" => "api" }
    }
  }
  
  # Autoscraper service logs
  if [service] == "autoscraper" or [container_name] =~ /autoscraper/ {
    grok {
      match => {
        "message" => "%{TIMESTAMP_ISO8601:timestamp} - %{WORD:log_level} - %{DATA:logger} - %{GREEDYDATA:log_message}"
      }
    }
    
    # Parse scraping metrics
    if [log_message] =~ /scraped|jobs|companies/ {
      grok {
        match => {
          "log_message" => "Scraped %{INT:jobs_scraped} jobs from %{INT:companies_scraped} companies in %{NUMBER:scrape_duration} seconds"
        }
      }
      
      if [jobs_scraped] {
        mutate {
          convert => {
            "jobs_scraped" => "integer"
            "companies_scraped" => "integer"
            "scrape_duration" => "float"
          }
          add_field => { "metric_type" => "scraping_stats" }
        }
      }
    }
    
    mutate {
      add_field => { "service_type" => "scraper" }
    }
  }
  
  # Admin Panel logs (Next.js)
  if [service] == "admin" or [container_name] =~ /admin/ {
    # Parse Next.js logs
    if [message] =~ /^\[/ {
      grok {
        match => {
          "message" => "\[%{TIMESTAMP_ISO8601:timestamp}\] %{WORD:log_level}: %{GREEDYDATA:log_message}"
        }
      }
    }
    
    # Parse API requests
    if [log_message] =~ /API|Request/ {
      grok {
        match => {
          "log_message" => "API %{WORD:api_method} %{URIPATH:api_path} - %{INT:status_code} - %{NUMBER:response_time}ms"
        }
      }
      
      if [status_code] {
        mutate {
          convert => {
            "status_code" => "integer"
            "response_time" => "float"
          }
        }
      }
    }
    
    mutate {
      add_field => { "service_type" => "frontend" }
    }
  }
  
  # Public Website logs (React/Vite)
  if [service] == "public" or [container_name] =~ /public/ {
    # Parse Vite dev server logs
    if [message] =~ /vite|Local:|Network:/ {
      mutate {
        add_field => { "log_type" => "dev_server" }
      }
    }
    
    mutate {
      add_field => { "service_type" => "frontend" }
    }
  }
  
  # Celery worker logs
  if [service] == "celery" or [container_name] =~ /celery/ {
    # Parse Celery task logs
    grok {
      match => {
        "message" => "\[%{TIMESTAMP_ISO8601:timestamp}\] \[%{WORD:log_level}/%{WORD:process_type}\] %{GREEDYDATA:log_message}"
      }
    }
    
    # Parse task execution
    if [log_message] =~ /Task|Received/ {
      grok {
        match => {
          "log_message" => "Task %{DATA:task_name}\[%{DATA:task_id}\] %{WORD:task_status}"
        }
      }
      
      if [task_name] {
        mutate {
          add_field => { "metric_type" => "task_execution" }
        }
      }
    }
    
    mutate {
      add_field => { "service_type" => "worker" }
    }
  }
  
  # Database logs (MongoDB)
  if [service] == "mongodb" or [container_name] =~ /mongo/ {
    # Parse MongoDB logs
    grok {
      match => {
        "message" => "%{TIMESTAMP_ISO8601:timestamp} %{WORD:severity} %{WORD:component} \[%{DATA:context}\] %{GREEDYDATA:log_message}"
      }
    }
    
    # Parse connection logs
    if [log_message] =~ /connection/ {
      grok {
        match => {
          "log_message" => "connection accepted from %{IPORHOST:client_ip}:%{INT:client_port}"
        }
      }
    }
    
    mutate {
      add_field => { "service_type" => "database" }
    }
  }
  
  # Redis logs
  if [service] == "redis" or [container_name] =~ /redis/ {
    # Parse Redis logs
    grok {
      match => {
        "message" => "%{INT:process_id}:%{WORD:role} %{TIMESTAMP_ISO8601:timestamp} %{WORD:log_level} %{GREEDYDATA:log_message}"
      }
    }
    
    mutate {
      add_field => { "service_type" => "cache" }
    }
  }
  
  # Nginx logs
  if [service] == "nginx" or [container_name] =~ /nginx/ {
    # Parse Nginx access logs
    if [message] !~ /^\d{4}/ {
      grok {
        match => {
          "message" => "%{IPORHOST:client_ip} - %{DATA:remote_user} \[%{HTTPDATE:access_time}\] \"%{WORD:http_method} %{URIPATH:uri_path}(?:%{URIPARAM:uri_params})? HTTP/%{NUMBER:http_version}\" %{INT:status_code} %{INT:response_size} \"%{DATA:referrer}\" \"%{DATA:user_agent}\" %{NUMBER:response_time}"
        }
      }
      
      mutate {
        convert => {
          "status_code" => "integer"
          "response_size" => "integer"
          "response_time" => "float"
        }
      }
    } else {
      # Parse Nginx error logs
      grok {
        match => {
          "message" => "%{TIMESTAMP_ISO8601:timestamp} \[%{WORD:log_level}\] %{INT:process_id}#%{INT:thread_id}: %{GREEDYDATA:log_message}"
        }
      }
    }
    
    mutate {
      add_field => { "service_type" => "proxy" }
    }
  }
  
  # Security logs processing
  if [log_message] =~ /failed|unauthorized|forbidden|attack|suspicious/ {
    mutate {
      add_field => { "security_event" => "true" }
      add_tag => [ "security" ]
    }
    
    # Extract IP addresses for security analysis
    if [client_ip] {
      geoip {
        source => "client_ip"
        target => "geoip"
      }
    }
  }
  
  # Performance metrics extraction
  if [response_time] {
    if [response_time] > 1000 {
      mutate {
        add_tag => [ "slow_response" ]
      }
    }
  }
  
  if [status_code] {
    if [status_code] >= 400 {
      mutate {
        add_tag => [ "error_response" ]
      }
    }
    
    if [status_code] >= 500 {
      mutate {
        add_tag => [ "server_error" ]
      }
    }
  }
  
  # Clean up fields
  mutate {
    remove_field => [ "[fields]", "[agent]", "[ecs]", "[host][architecture]", "[host][os]" ]
  }
  
  # Convert log levels to lowercase
  if [log_level] {
    mutate {
      lowercase => [ "log_level" ]
    }
  }
  
  # Add index pattern based on service type
  if [service_type] {
    mutate {
      add_field => { "[@metadata][index]" => "remotehive-%{service_type}-%{+YYYY.MM.dd}" }
    }
  } else {
    mutate {
      add_field => { "[@metadata][index]" => "remotehive-general-%{+YYYY.MM.dd}" }
    }
  }
}

output {
  # Output to Elasticsearch
  elasticsearch {
    hosts => ["${ELASTICSEARCH_HOSTS:elasticsearch:9200}"]
    index => "%{[@metadata][index]}"
    
    # Authentication if required
    # user => "${ELASTICSEARCH_USERNAME}"
    # password => "${ELASTICSEARCH_PASSWORD}"
    
    # Template management
    template_name => "remotehive"
    template_pattern => "remotehive-*"
    template => "/usr/share/logstash/templates/remotehive-template.json"
    template_overwrite => true
    
    # Document ID for deduplication
    document_id => "%{[@metadata][fingerprint]}"
  }
  
  # Output to file for debugging (conditional)
  if [environment] == "development" {
    file {
      path => "/var/log/logstash/debug-%{service_type}-%{+YYYY-MM-dd}.log"
      codec => json_lines
    }
  }
  
  # Output security events to separate index
  if "security" in [tags] {
    elasticsearch {
      hosts => ["${ELASTICSEARCH_HOSTS:elasticsearch:9200}"]
      index => "remotehive-security-%{+YYYY.MM.dd}"
    }
  }
  
  # Output metrics to InfluxDB (if configured)
  if [metric_type] {
    influxdb {
      host => "${INFLUXDB_HOST:influxdb}"
      port => "${INFLUXDB_PORT:8086}"
      db => "${INFLUXDB_DATABASE:remotehive_metrics}"
      measurement => "%{metric_type}"
      
      # Authentication if required
      # user => "${INFLUXDB_USERNAME}"
      # password => "${INFLUXDB_PASSWORD}"
    }
  }
  
  # Output to stdout for debugging
  if [environment] == "development" and "debug" in [tags] {
    stdout {
      codec => rubydebug
    }
  }
}